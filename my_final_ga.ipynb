{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/spetz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np \n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from deap import algorithms, base, creator, tools\n",
    "from nltk import word_tokenize\n",
    "import re, string\n",
    "import random\n",
    "import statistics \n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_freq = {}  # frequency map for words in list\n",
    "dict_paired_freq = {}\n",
    "\n",
    "path = '/home/spetz/Downloads/DeliciousMIL/Data/train-data.dat'\n",
    "clean_files = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "file = open(path).readlines()\n",
    "len(file)\n",
    "\n",
    "#clear data\n",
    "clear_file=[]\n",
    "for i in range(len(file)):\n",
    "    x=re.sub('<.*?>','',file[i])\n",
    "    clear_file.append(x)\n",
    "\n",
    "clear_file=clear_file[:]\n",
    "\n",
    "#perasma tou clear keimenou sto words string\n",
    "words = ''\n",
    "for line in clear_file:\n",
    "    words += line\n",
    "\n",
    "\n",
    "tokenized_words = word_tokenize(words) # list of all words in new dictionary\n",
    "WORD_LIST = list(set(tokenized_words)) # create a set out of words so there are no repeats in word list and make list again\n",
    "dictionary_size = len(WORD_LIST) #6853\n",
    "\n",
    "\n",
    "#Dhmiourgia dictionary lekseis kai suxnotitas emfanisi tis lekseis se olo to keimeno\n",
    "for word in tokenized_words: #gia kathe word sto lista leksewn olou tou keimenou\n",
    "    if word not in dict_freq: \n",
    "        dict_freq[word] = 1\n",
    "    else:\n",
    "        dict_freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8520"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF = (Frequency of the word in the sentence) / (Total number of words in the sentence)\n",
    "def computeTF(wordDict,bow):\n",
    "    tfDict={}\n",
    "    bowCount=len(bow)\n",
    "    for word,count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "tf=computeTF(dict_freq,tokenized_words)\n",
    "\n",
    "#IDF: log((Total number of sentences (documents))/(Number of sentences (documents) containing the word))\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(dict_freq,0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return idfDict\n",
    "\n",
    "idf=computeIDF(dict_freq)\n",
    "len(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL TF-IDF\n",
    "def computeDFIDF(tfbow,idfs):\n",
    "    tfidf={}\n",
    "    for word,val in tfbow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final TF-IDF\n",
    "Tf_idf=computeDFIDF(tf,idf)\n",
    "#Apothikeusi mono ton timwn\n",
    "new_list = list(Tf_idf.values())\n",
    "len(new_list)\n",
    "tf_idf_Xtrain = np.asarray(new_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a FitnessMax class \n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "#Create an Individual class \n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "def getFitness(individual):\n",
    "    counter = 0\n",
    "    fitness = 0\n",
    "\n",
    "    #gia kathe thesi sto indi\n",
    "    for word_index in individual:\n",
    "        #vale sto string thn antistoixi leksi\n",
    "        if new_list[word_index] <= 1:\n",
    "            fitness += new_list[word_index]\n",
    "        \n",
    "        else:\n",
    "            fitness -=20\n",
    "    \n",
    "    return fitness,\n",
    "\n",
    "        \n",
    "#The toolbox is a container for functions with their arguments\n",
    "toolbox = base.Toolbox()\n",
    "# Attribute generator \n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "# Structure initializers\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, len(new_list))\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual) \n",
    "# Register the genetic operators\t\n",
    "toolbox.register(\"evaluate\", getFitness)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ITERATION \n",
      "-- Generation 1 --\n",
      " Max fitness:15.591211973195517 && Average fitness:15.542232837828829\n",
      "-- Generation 2 --\n",
      " Max fitness:15.60428224808269 && Average fitness:15.569233274108914\n",
      "-- Generation 3 --\n",
      " Max fitness:15.60840970331022 && Average fitness:15.586052654161099\n",
      "-- Generation 4 --\n",
      " Max fitness:15.615288795356102 && Average fitness:15.591831091479634\n",
      "-- Generation 5 --\n",
      " Max fitness:15.629046979447871 && Average fitness:15.598778974445981\n",
      "-- Generation 6 --\n",
      " Max fitness:15.6331744346754 && Average fitness:15.606105207474846\n",
      "-- Generation 7 --\n",
      " Max fitness:15.670321531723168 && Average fitness:15.620620091691663\n",
      "-- Generation 8 --\n",
      " Max fitness:15.671009440927756 && Average fitness:15.636545189777882\n",
      "-- Generation 9 --\n",
      " Max fitness:15.671009440927756 && Average fitness:15.645900754960284\n",
      "-- Generation 10 --\n",
      " Max fitness:15.684079715814926 && Average fitness:15.660828384699846\n",
      "-- Generation 11 --\n",
      " Max fitness:15.697837899906695 && Average fitness:15.668395385950319\n",
      "-- Generation 12 --\n",
      " Max fitness:15.696462081497518 && Average fitness:15.670046368041332\n",
      "-- Generation 13 --\n",
      " Max fitness:15.696462081497518 && Average fitness:15.679057978621435\n",
      "-- Generation 14 --\n",
      " Max fitness:15.700589536725046 && Average fitness:15.686074652508239\n",
      "-- Generation 15 --\n",
      " Max fitness:15.703341173543398 && Average fitness:15.692540999031365\n",
      "-- Generation 16 --\n",
      " Max fitness:15.703341173543398 && Average fitness:15.694123190201916\n",
      "-- Generation 17 --\n",
      " Max fitness:15.703341173543398 && Average fitness:15.695567799531549\n",
      "-- Generation 18 --\n",
      " Max fitness:15.710220265589282 && Average fitness:15.697941086287383\n",
      "-- Generation 19 --\n",
      " Max fitness:15.715035630021397 && Average fitness:15.702515682497893\n",
      "-- Generation 20 --\n",
      " Max fitness:15.715035630021397 && Average fitness:15.706608742265194\n",
      "-- Generation 21 --\n",
      " Max fitness:15.716411448430573 && Average fitness:15.708053351594828\n",
      "-- Generation 22 --\n",
      " Max fitness:15.718475176044342 && Average fitness:15.708672469878957\n",
      "-- Generation 23 --\n",
      " Max fitness:15.719850994453514 && Average fitness:15.711733665839372\n",
      "-- Generation 24 --\n",
      " Max fitness:15.723978449681045 && Average fitness:15.713831788913367\n",
      "-- Generation 25 --\n",
      " Max fitness:15.726042177294808 && Average fitness:15.715035630021399\n",
      "-- Generation 26 --\n",
      "<-------------------ITERATION 1 COMPLETED ----------------->\n",
      " Max fitness:15.727417995703988 && Average fitness:15.719231876169385\n",
      "2 ITERATION \n",
      "-- Generation 1 --\n",
      " Max fitness:15.599466883650557 && Average fitness:15.559877708926521\n",
      "-- Generation 2 --\n",
      " Max fitness:15.631110707061636 && Average fitness:15.573979847620574\n",
      "-- Generation 3 --\n",
      " Max fitness:15.631110707061636 && Average fitness:15.582750689979074\n",
      "-- Generation 4 --\n",
      " Max fitness:15.648308437176324 && Average fitness:15.595958546707166\n",
      "-- Generation 5 --\n",
      " Max fitness:15.643493072744223 && Average fitness:15.60964793987848\n",
      "-- Generation 6 --\n",
      " Max fitness:15.660002893654335 && Average fitness:15.626604901771577\n",
      "-- Generation 7 --\n",
      " Max fitness:15.673073168541508 && Average fitness:15.636854748919939\n",
      "-- Generation 8 --\n",
      " Max fitness:15.678576442178217 && Average fitness:15.64892755546046\n",
      "-- Generation 9 --\n",
      " Max fitness:15.678576442178217 && Average fitness:15.656838511313225\n",
      "-- Generation 10 --\n",
      " Max fitness:15.686143443428687 && Average fitness:15.665024630847828\n",
      "-- Generation 11 --\n",
      " Max fitness:15.686831352633277 && Average fitness:15.67211009565509\n",
      "-- Generation 12 --\n",
      " Max fitness:15.70127744592963 && Average fitness:15.678026114814552\n",
      "-- Generation 13 --\n",
      " Max fitness:15.697149990702101 && Average fitness:15.685937070667313\n",
      "-- Generation 14 --\n",
      " Max fitness:15.702653264338808 && Average fitness:15.686246629809379\n",
      "-- Generation 15 --\n",
      " Max fitness:15.707468628770926 && Average fitness:15.690614853258515\n",
      "-- Generation 16 --\n",
      " Max fitness:15.706780719566337 && Average fitness:15.693916817440542\n",
      "-- Generation 17 --\n",
      " Max fitness:15.70678071956634 && Average fitness:15.697906690827153\n",
      "-- Generation 18 --\n",
      " Max fitness:15.721914722067284 && Average fitness:15.703375569003631\n",
      "-- Generation 19 --\n",
      " Max fitness:15.721914722067284 && Average fitness:15.708225328895972\n",
      "-- Generation 20 --\n",
      " Max fitness:15.721914722067284 && Average fitness:15.702343705196744\n",
      "-- Generation 21 --\n",
      " Max fitness:15.722602631271867 && Average fitness:15.710117079208596\n",
      "-- Generation 22 --\n",
      " Max fitness:15.722602631271872 && Average fitness:15.713625416151993\n",
      "-- Generation 23 --\n",
      " Max fitness:15.72604217729481 && Average fitness:15.7162050756692\n",
      "-- Generation 24 --\n",
      " Max fitness:15.739112452181988 && Average fitness:15.71950703985122\n",
      "-- Generation 25 --\n",
      " Max fitness:15.743927816614105 && Average fitness:15.729722491539363\n",
      "-- Generation 26 --\n",
      "<-------------------ITERATION 2 COMPLETED ----------------->\n",
      " Max fitness:15.746679453432462 && Average fitness:15.730891937187156\n",
      "3 ITERATION \n",
      "-- Generation 1 --\n",
      " Max fitness:15.578829607512919 && Average fitness:15.539206037328636\n",
      "-- Generation 2 --\n",
      " Max fitness:15.612537158537739 && Average fitness:15.561047154574315\n",
      "-- Generation 3 --\n",
      " Max fitness:15.612537158537739 && Average fitness:15.573188752035296\n",
      "-- Generation 4 --\n",
      " Max fitness:15.631110707061632 && Average fitness:15.59705920143451\n",
      "-- Generation 5 --\n",
      " Max fitness:15.633862343879983 && Average fitness:15.613362649583248\n",
      "-- Generation 6 --\n",
      " Max fitness:15.64624470956257 && Average fitness:15.623715683112303\n",
      "-- Generation 7 --\n",
      " Max fitness:15.65725125683598 && Average fitness:15.628806211226262\n",
      "-- Generation 8 --\n",
      " Max fitness:15.65725125683598 && Average fitness:15.637989799107515\n",
      "-- Generation 9 --\n",
      " Max fitness:15.66619407649563 && Average fitness:15.646485477784177\n",
      "-- Generation 10 --\n",
      " Max fitness:15.686831352633272 && Average fitness:15.655325111063135\n",
      "-- Generation 11 --\n",
      " Max fitness:15.687519261837862 && Average fitness:15.659934102733875\n",
      "-- Generation 12 --\n",
      " Max fitness:15.693022535474567 && Average fitness:15.666572426558153\n",
      "-- Generation 13 --\n",
      " Max fitness:15.687519261837863 && Average fitness:15.67345151860403\n",
      "-- Generation 14 --\n",
      " Max fitness:15.701277445929627 && Average fitness:15.678473255797524\n",
      "-- Generation 15 --\n",
      " Max fitness:15.701277445929627 && Average fitness:15.681912801820468\n",
      "-- Generation 16 --\n",
      " Max fitness:15.701277445929627 && Average fitness:15.68817277558222\n",
      "-- Generation 17 --\n",
      " Max fitness:15.706092810361744 && Average fitness:15.694467144804202\n",
      "-- Generation 18 --\n",
      " Max fitness:15.705404901157156 && Average fitness:15.697872295366917\n",
      "-- Generation 19 --\n",
      " Max fitness:15.724666358885619 && Average fitness:15.700830304946646\n",
      "-- Generation 20 --\n",
      " Max fitness:15.730857541726918 && Average fitness:15.702928428020638\n",
      "-- Generation 21 --\n",
      " Max fitness:15.730857541726918 && Average fitness:15.70224051881605\n",
      "-- Generation 22 --\n",
      " Max fitness:15.732921269340682 && Average fitness:15.70970433368583\n",
      "-- Generation 23 --\n",
      " Max fitness:15.732921269340682 && Average fitness:15.717684080459055\n",
      "-- Generation 24 --\n",
      " Max fitness:15.73360917854527 && Average fitness:15.721811535686584\n",
      "-- Generation 25 --\n",
      " Max fitness:15.739112452181978 && Average fitness:15.722189885749106\n",
      "-- Generation 26 --\n",
      "<-------------------ITERATION 3 COMPLETED ----------------->\n",
      " Max fitness:15.739112452181978 && Average fitness:15.726145363675494\n",
      "4 ITERATION \n",
      "-- Generation 1 --\n",
      " Max fitness:15.576765879899156 && Average fitness:15.542473606050432\n",
      "-- Generation 2 --\n",
      " Max fitness:15.602218520468925 && Average fitness:15.561941436540286\n",
      "-- Generation 3 --\n",
      " Max fitness:15.602218520468925 && Average fitness:15.575974784313885\n",
      "-- Generation 4 --\n",
      " Max fitness:15.617352522969865 && Average fitness:15.584195299308714\n",
      "-- Generation 5 --\n",
      " Max fitness:15.617352522969865 && Average fitness:15.599260510889192\n",
      "-- Generation 6 --\n",
      " Max fitness:15.617352522969865 && Average fitness:15.605726857412318\n",
      "-- Generation 7 --\n",
      " Max fitness:15.621479978197396 && Average fitness:15.614979236214035\n",
      "-- Generation 8 --\n",
      " Max fitness:15.624919524220338 && Average fitness:15.616389450083442\n",
      "-- Generation 9 --\n",
      " Max fitness:15.632486525470808 && Average fitness:15.61728373204941\n",
      "-- Generation 10 --\n",
      " Max fitness:15.629734888652454 && Average fitness:15.618625154998352\n",
      "-- Generation 11 --\n",
      " Max fitness:15.629734888652454 && Average fitness:15.619691414265466\n",
      "-- Generation 12 --\n",
      " Max fitness:15.640741435925866 && Average fitness:15.625848201646534\n",
      "-- Generation 13 --\n",
      " Max fitness:15.640741435925866 && Average fitness:15.629631702271766\n",
      "-- Generation 14 --\n",
      " Max fitness:15.640741435925866 && Average fitness:15.633449598357231\n",
      "-- Generation 15 --\n",
      " Max fitness:15.645556800357987 && Average fitness:15.635650907811916\n",
      "-- Generation 16 --\n",
      " Max fitness:15.645556800357987 && Average fitness:15.639228035675774\n",
      "-- Generation 17 --\n",
      " Max fitness:15.65587543842681 && Average fitness:15.641154181448622\n",
      "-- Generation 18 --\n",
      " Max fitness:15.658627075245162 && Average fitness:15.644628122931795\n",
      "-- Generation 19 --\n",
      " Max fitness:15.648996346380928 && Average fitness:15.643802631886288\n",
      "-- Generation 20 --\n",
      " Max fitness:15.651747983199284 && Average fitness:15.64424977286927\n",
      "-- Generation 21 --\n",
      " Max fitness:15.65243589240387 && Average fitness:15.643286699982847\n",
      "-- Generation 22 --\n",
      " Max fitness:15.65725125683599 && Average fitness:15.647895691653591\n",
      "-- Generation 23 --\n",
      " Max fitness:15.667569894904812 && Average fitness:15.647654923431986\n",
      "-- Generation 24 --\n",
      " Max fitness:15.667569894904812 && Average fitness:15.649443487363914\n",
      "-- Generation 25 --\n",
      " Max fitness:15.67169735013234 && Average fitness:15.656460161250715\n",
      "-- Generation 26 --\n",
      "<-------------------ITERATION 4 COMPLETED ----------------->\n",
      " Max fitness:15.672385259336929 && Average fitness:15.663820789739805\n",
      "5 ITERATION \n",
      "-- Generation 1 --\n",
      " Max fitness:15.585708699558806 && Average fitness:15.528715421958669\n",
      "-- Generation 2 --\n",
      " Max fitness:15.592587791604688 && Average fitness:15.549283907175859\n",
      "-- Generation 3 --\n",
      " Max fitness:15.604970157287273 && Average fitness:15.571434583563601\n",
      "-- Generation 4 --\n",
      " Max fitness:15.62835907024328 && Average fitness:15.586706167905456\n",
      "-- Generation 5 --\n",
      " Max fitness:15.62835907024328 && Average fitness:15.601736984025711\n",
      "-- Generation 6 --\n",
      " Max fitness:15.62835907024328 && Average fitness:15.61374099964578\n",
      "-- Generation 7 --\n",
      " Max fitness:15.633862343879985 && Average fitness:15.619485041504088\n",
      "-- Generation 8 --\n",
      " Max fitness:15.646244709562572 && Average fitness:15.626639297231808\n",
      "-- Generation 9 --\n",
      " Max fitness:15.646244709562572 && Average fitness:15.6310763116014\n",
      "-- Generation 10 --\n",
      " Max fitness:15.647620527971748 && Average fitness:15.635719698732373\n",
      "-- Generation 11 --\n",
      " Max fitness:15.657939166040569 && Average fitness:15.642186045255505\n",
      "-- Generation 12 --\n",
      " Max fitness:15.666194076495628 && Average fitness:15.646244709562572\n",
      "-- Generation 13 --\n",
      " Max fitness:15.671009440927744 && Average fitness:15.65346775621075\n",
      "-- Generation 14 --\n",
      " Max fitness:15.671009440927744 && Average fitness:15.655944229347261\n",
      "-- Generation 15 --\n",
      " Max fitness:15.671009440927747 && Average fitness:15.661000362000985\n",
      "-- Generation 16 --\n",
      " Max fitness:15.673761077746097 && Average fitness:15.666159681035396\n",
      "-- Generation 17 --\n",
      " Max fitness:15.679264351382805 && Average fitness:15.665299794529659\n",
      "-- Generation 18 --\n",
      " Max fitness:15.679264351382805 && Average fitness:15.668842526933293\n",
      "-- Generation 19 --\n",
      " Max fitness:15.68270389740575 && Average fitness:15.67379547320633\n",
      "-- Generation 20 --\n",
      " Max fitness:15.68270389740575 && Average fitness:15.673382727683574\n",
      "-- Generation 21 --\n",
      " Max fitness:15.69233462626998 && Average fitness:15.677303810149729\n",
      "-- Generation 22 --\n",
      " Max fitness:15.694398353883749 && Average fitness:15.681465660837492\n",
      "-- Generation 23 --\n",
      " Max fitness:15.699213718315862 && Average fitness:15.687072120854884\n",
      "-- Generation 24 --\n",
      " Max fitness:15.697837899906686 && Average fitness:15.68851673018452\n",
      "-- Generation 25 --\n",
      " Max fitness:15.715723539225989 && Average fitness:15.691784298906313\n",
      "-- Generation 26 --\n",
      "<-------------------ITERATION 5 COMPLETED ----------------->\n",
      " Max fitness:15.715723539225989 && Average fitness:15.699729650219306\n",
      "6 ITERATION \n",
      "-- Generation 1 --\n",
      " Max fitness:15.574702152285392 && Average fitness:15.548939952573562\n",
      "-- Generation 2 --\n",
      " Max fitness:15.59396361001385 && Average fitness:15.560118477148118\n",
      "-- Generation 3 --\n",
      " Max fitness:15.59396361001385 && Average fitness:15.566963173733773\n",
      "-- Generation 4 --\n",
      " Max fitness:15.606345975696447 && Average fitness:15.575871597933187\n",
      "-- Generation 5 --\n",
      " Max fitness:15.606345975696447 && Average fitness:15.590352086689768\n",
      "-- Generation 6 --\n",
      " Max fitness:15.606345975696447 && Average fitness:15.596646455911758\n",
      "-- Generation 7 --\n",
      " Max fitness:15.608409703310212 && Average fitness:15.602321706849608\n",
      "-- Generation 8 --\n",
      " Max fitness:15.618728341379034 && Average fitness:15.605554880111168\n",
      "-- Generation 9 --\n",
      " Max fitness:15.618728341379034 && Average fitness:15.608306516929526\n",
      "-- Generation 10 --\n",
      " Max fitness:15.635926071493744 && Average fitness:15.614463304310586\n",
      "-- Generation 11 --\n",
      " Max fitness:15.635926071493744 && Average fitness:15.618006036714215\n",
      "-- Generation 12 --\n",
      " Max fitness:15.642805163539625 && Average fitness:15.623922055873674\n",
      "-- Generation 13 --\n",
      " Max fitness:15.645556800357978 && Average fitness:15.62787753380006\n",
      "-- Generation 14 --\n",
      " Max fitness:15.653811710813038 && Average fitness:15.632486525470805\n",
      "-- Generation 15 --\n",
      " Max fitness:15.659314984449745 && Average fitness:15.639812758499668\n",
      "-- Generation 16 --\n",
      " Max fitness:15.672385259336915 && Average fitness:15.65126644675606\n",
      "-- Generation 17 --\n",
      " Max fitness:15.672385259336915 && Average fitness:15.656116206648406\n",
      "-- Generation 18 --\n",
      " Max fitness:15.678576442178208 && Average fitness:15.659108611688364\n",
      "-- Generation 19 --\n",
      " Max fitness:15.693022535474563 && Average fitness:15.66822340864916\n",
      "-- Generation 20 --\n",
      " Max fitness:15.705404901157157 && Average fitness:15.676512714564447\n",
      "-- Generation 21 --\n",
      " Max fitness:15.705404901157157 && Average fitness:15.684939602320654\n",
      "-- Generation 22 --\n",
      " Max fitness:15.708844447180097 && Average fitness:15.6915435306847\n",
      "-- Generation 23 --\n",
      " Max fitness:15.708844447180097 && Average fitness:15.69752834076462\n",
      "-- Generation 24 --\n",
      " Max fitness:15.71503563002139 && Average fitness:15.702171727895593\n",
      "-- Generation 25 --\n",
      " Max fitness:15.710908174793863 && Average fitness:15.701587005071687\n",
      "-- Generation 26 --\n",
      "<-------------------ITERATION 6 COMPLETED ----------------->\n",
      " Max fitness:15.713659811612217 && Average fitness:15.705129737475318\n",
      "7 ITERATION \n",
      "-- Generation 1 --\n",
      " Max fitness:15.552001148533991 && Average fitness:15.513512628537276\n",
      "-- Generation 2 --\n",
      " Max fitness:15.580893335126696 && Average fitness:15.534081113754464\n",
      "-- Generation 3 --\n",
      " Max fitness:15.580893335126696 && Average fitness:15.55492476265349\n",
      "-- Generation 4 --\n",
      " Max fitness:15.600154792855168 && Average fitness:15.575596434251366\n",
      "-- Generation 5 --\n",
      " Max fitness:15.60359433887811 && Average fitness:15.585777490479273\n",
      "-- Generation 6 --\n",
      " Max fitness:15.60703388490105 && Average fitness:15.589492200184054\n",
      "-- Generation 7 --\n",
      " Max fitness:15.609785521719408 && Average fitness:15.594892287440066\n",
      "-- Generation 8 --\n",
      " Max fitness:15.61184924933317 && Average fitness:15.601324238502965\n",
      "-- Generation 9 --\n",
      " Max fitness:15.616664613765293 && Average fitness:15.603972688940633\n",
      "-- Generation 10 --\n",
      " Max fitness:15.624231615015754 && Average fitness:15.610026289941013\n",
      "-- Generation 11 --\n",
      " Max fitness:15.63661398069835 && Average fitness:15.615667145418637\n",
      "-- Generation 12 --\n",
      " Max fitness:15.643493072744231 && Average fitness:15.619381855123413\n",
      "-- Generation 13 --\n",
      " Max fitness:15.643493072744231 && Average fitness:15.628875002146728\n",
      "-- Generation 14 --\n",
      " Max fitness:15.651060073994692 && Average fitness:15.635754094192611\n",
      "-- Generation 15 --\n",
      " Max fitness:15.657251256835995 && Average fitness:15.641738904272529\n",
      "-- Generation 16 --\n",
      " Max fitness:15.66000289365435 && Average fitness:15.644077795568128\n",
      "-- Generation 17 --\n",
      " Max fitness:15.66000289365435 && Average fitness:15.647930087113826\n",
      "-- Generation 18 --\n",
      " Max fitness:15.671009440927753 && Average fitness:15.655015551921082\n",
      "-- Generation 19 --\n",
      " Max fitness:15.676512714564463 && Average fitness:15.657044884074613\n",
      "-- Generation 20 --\n",
      " Max fitness:15.676512714564463 && Average fitness:15.662032225807883\n",
      "-- Generation 21 --\n",
      " Max fitness:15.680640169791996 && Average fitness:15.666228471955867\n",
      "-- Generation 22 --\n",
      " Max fitness:15.686143443428698 && Average fitness:15.670011972581102\n",
      "-- Generation 23 --\n",
      " Max fitness:15.69302253547458 && Average fitness:15.674861732473454\n",
      "-- Generation 24 --\n",
      " Max fitness:15.69302253547458 && Average fitness:15.68136247445681\n",
      "-- Generation 25 --\n",
      " Max fitness:15.69646208149752 && Average fitness:15.682015988201169\n",
      "-- Generation 26 --\n",
      "<-------------------ITERATION 7 COMPLETED ----------------->\n",
      " Max fitness:15.699901627520461 && Average fitness:15.685937070667318\n",
      "8 ITERATION \n",
      "-- Generation 1 --\n",
      " Max fitness:15.607033884901037 && Average fitness:15.555819044619446\n",
      "-- Generation 2 --\n",
      " Max fitness:15.630422797857037 && Average fitness:15.58226915353587\n",
      "-- Generation 3 --\n",
      " Max fitness:15.6386777083121 && Average fitness:15.602287311389379\n",
      "-- Generation 4 --\n",
      " Max fitness:15.641429345130454 && Average fitness:15.618384386776743\n",
      "-- Generation 5 --\n",
      " Max fitness:15.677888532973626 && Average fitness:15.628668629385341\n",
      "-- Generation 6 --\n",
      " Max fitness:15.686143443428689 && Average fitness:15.648824369079772\n",
      "-- Generation 7 --\n",
      " Max fitness:15.687519261837865 && Average fitness:15.664543094404612\n",
      "-- Generation 8 --\n",
      " Max fitness:15.688207171042453 && Average fitness:15.673279541302886\n",
      "-- Generation 9 --\n",
      " Max fitness:15.688207171042453 && Average fitness:15.681637638138634\n",
      "-- Generation 10 --\n",
      " Max fitness:15.688895080247041 && Average fitness:15.680330610649918\n",
      "-- Generation 11 --\n",
      " Max fitness:15.688895080247041 && Average fitness:15.681878406360237\n",
      "-- Generation 12 --\n",
      " Max fitness:15.688895080247041 && Average fitness:15.68235994280345\n",
      "-- Generation 13 --\n",
      " Max fitness:15.688895080247041 && Average fitness:15.6866249798719\n",
      "-- Generation 14 --\n",
      " Max fitness:15.688895080247041 && Average fitness:15.68483641593997\n",
      "-- Generation 15 --\n",
      " Max fitness:15.723978449681042 && Average fitness:15.682463129184143\n",
      "-- Generation 16 --\n",
      " Max fitness:15.723978449681042 && Average fitness:15.690614853258513\n",
      "-- Generation 17 --\n",
      " Max fitness:15.723978449681042 && Average fitness:15.70062393218527\n",
      "-- Generation 18 --\n",
      " Max fitness:15.723978449681042 && Average fitness:15.707468628770922\n",
      "-- Generation 19 --\n",
      " Max fitness:15.723978449681042 && Average fitness:15.714622884498644\n",
      "-- Generation 20 --\n",
      " Max fitness:15.728793814113159 && Average fitness:15.72081406733994\n",
      "-- Generation 21 --\n",
      " Max fitness:15.728793814113159 && Average fitness:15.719850994453514\n",
      "-- Generation 22 --\n",
      " Max fitness:15.728793814113159 && Average fitness:15.717443312237453\n",
      "-- Generation 23 --\n",
      " Max fitness:15.729481723317747 && Average fitness:15.720332530896723\n",
      "-- Generation 24 --\n",
      " Max fitness:15.732921269340688 && Average fitness:15.72108923102177\n",
      "-- Generation 25 --\n",
      " Max fitness:15.732921269340688 && Average fitness:15.722568235811632\n",
      "-- Generation 26 --\n",
      "<-------------------ITERATION 8 COMPLETED ----------------->\n",
      " Max fitness:15.734297087749864 && Average fitness:15.719885389913745\n",
      "9 ITERATION \n",
      "-- Generation 1 --\n",
      " Max fitness:15.58777242717257 && Average fitness:15.555131135414857\n",
      "-- Generation 2 --\n",
      " Max fitness:15.593963610013864 && Average fitness:15.564073955074505\n",
      "-- Generation 3 --\n",
      " Max fitness:15.605658066491864 && Average fitness:15.578416861990169\n",
      "-- Generation 4 --\n",
      " Max fitness:15.616664613765277 && Average fitness:15.59286295528652\n",
      "-- Generation 5 --\n",
      " Max fitness:15.616664613765277 && Average fitness:15.59410119185478\n",
      "-- Generation 6 --\n",
      " Max fitness:15.63592607149375 && Average fitness:15.607756189565862\n",
      "-- Generation 7 --\n",
      " Max fitness:15.65449962001763 && Average fitness:15.613947372407154\n",
      "-- Generation 8 --\n",
      " Max fitness:15.65449962001763 && Average fitness:15.621892723720148\n",
      "-- Generation 9 --\n",
      " Max fitness:15.655875438426806 && Average fitness:15.629356538589926\n",
      "-- Generation 10 --\n",
      " Max fitness:15.659314984449745 && Average fitness:15.636442003397187\n",
      "-- Generation 11 --\n",
      " Max fitness:15.67720062376904 && Average fitness:15.648549205397938\n",
      "-- Generation 12 --\n",
      " Max fitness:15.678576442178215 && Average fitness:15.659762125432726\n",
      "-- Generation 13 --\n",
      " Max fitness:15.67720062376904 && Average fitness:15.6661596810354\n",
      "-- Generation 14 --\n",
      " Max fitness:15.684079715814924 && Average fitness:15.670424718103842\n",
      "-- Generation 15 --\n",
      " Max fitness:15.690958807860804 && Average fitness:15.672247677496003\n",
      "-- Generation 16 --\n",
      " Max fitness:15.690958807860804 && Average fitness:15.67558403713826\n",
      "-- Generation 17 --\n",
      " Max fitness:15.704029082747985 && Average fitness:15.68246312918414\n",
      "-- Generation 18 --\n",
      " Max fitness:15.701277445929628 && Average fitness:15.684595647718368\n",
      "-- Generation 19 --\n",
      " Max fitness:15.710220265589273 && Average fitness:15.685111579621807\n",
      "-- Generation 20 --\n",
      " Max fitness:15.714347720816804 && Average fitness:15.691302762463101\n",
      "-- Generation 21 --\n",
      " Max fitness:15.714347720816804 && Average fitness:15.699351300156783\n",
      "-- Generation 22 --\n",
      " Max fitness:15.714347720816804 && Average fitness:15.702137332435361\n",
      "-- Generation 23 --\n",
      " Max fitness:15.714347720816804 && Average fitness:15.706574346804956\n",
      "-- Generation 24 --\n",
      " Max fitness:15.714347720816804 && Average fitness:15.706952696867486\n",
      "-- Generation 25 --\n",
      " Max fitness:15.734297087749866 && Average fitness:15.715035630021399\n",
      "-- Generation 26 --\n",
      "<-------------------ITERATION 9 COMPLETED ----------------->\n",
      " Max fitness:15.741176179795746 && Average fitness:15.712352784123501\n",
      "10 ITERATION \n",
      "-- Generation 1 --\n",
      " Max fitness:15.570574697057852 && Average fitness:15.539653178311621\n",
      "-- Generation 2 --\n",
      " Max fitness:15.570574697057852 && Average fitness:15.553101803261319\n",
      "-- Generation 3 --\n",
      " Max fitness:15.591899882400085 && Average fitness:15.559327381562841\n",
      "-- Generation 4 --\n",
      " Max fitness:15.600842702059733 && Average fitness:15.576077970694561\n",
      "-- Generation 5 --\n",
      " Max fitness:15.600842702059733 && Average fitness:15.588322754536225\n",
      "-- Generation 6 --\n",
      " Max fitness:15.600154792855156 && Average fitness:15.588632313678293\n",
      "-- Generation 7 --\n",
      " Max fitness:15.620792068992788 && Average fitness:15.593275700809272\n",
      "-- Generation 8 --\n",
      " Max fitness:15.620792068992788 && Average fitness:15.601874565866618\n",
      "-- Generation 9 --\n",
      " Max fitness:15.630422797857022 && Average fitness:15.614566490691269\n",
      "-- Generation 10 --\n",
      " Max fitness:15.631110707061614 && Average fitness:15.623922055873663\n",
      "-- Generation 11 --\n",
      " Max fitness:15.63730188990291 && Average fitness:15.627533579197754\n",
      "-- Generation 12 --\n",
      " Max fitness:15.642805163539613 && Average fitness:15.630422797857019\n",
      "-- Generation 13 --\n",
      " Max fitness:15.637301889902908 && Average fitness:15.630457193317255\n",
      "-- Generation 14 --\n",
      " Max fitness:15.649684255585493 && Average fitness:15.634309484862948\n",
      "-- Generation 15 --\n",
      " Max fitness:15.661378712063497 && Average fitness:15.636166839715338\n",
      "-- Generation 16 --\n",
      " Max fitness:15.661378712063497 && Average fitness:15.641050995067918\n",
      "-- Generation 17 --\n",
      " Max fitness:15.661378712063497 && Average fitness:15.649443487363888\n",
      "-- Generation 18 --\n",
      " Max fitness:15.661378712063497 && Average fitness:15.652091937801554\n",
      "-- Generation 19 --\n",
      " Max fitness:15.664818258086443 && Average fitness:15.656391370330235\n",
      "-- Generation 20 --\n",
      " Max fitness:15.66756989490479 && Average fitness:15.659865311813405\n",
      "-- Generation 21 --\n",
      " Max fitness:15.673073168541501 && Average fitness:15.661585084824878\n",
      "-- Generation 22 --\n",
      " Max fitness:15.679264351382795 && Average fitness:15.667432313063879\n",
      "-- Generation 23 --\n",
      " Max fitness:15.679952260587383 && Average fitness:15.674552173331364\n",
      "-- Generation 24 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Max fitness:15.692334626269977 && Average fitness:15.673864264126774\n",
      "-- Generation 25 --\n",
      " Max fitness:15.692334626269977 && Average fitness:15.67806051027477\n",
      "-- Generation 26 --\n",
      "<-------------------ITERATION 10 COMPLETED ----------------->\n",
      " Max fitness:15.69508626308833 && Average fitness:15.681878406360232\n",
      "<------------------------- APOTELESMATA ------------------------>\n",
      "Avg for generations: 14.461538461538462\n",
      "Best fitness of best individual: 15.746679453432462\n",
      "Avg fitness of best individual: 15.738554187404418\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution of best individual: [0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "population_size=20\n",
    "prob_cross = 0.6\n",
    "prob_mutation = 0.2\n",
    "genererations = 10\n",
    "\n",
    "averege_Fitness_all_gen=list()\n",
    "maxFitness_all_gen=list()\n",
    "Howmanygenerations =list()\n",
    "BestFitnessForAllGens =list()\n",
    "BestFitnessPerGens =list()\n",
    "BestFitness = list()\n",
    "genbest=list()\n",
    "best={}\n",
    "final_Bestfitness=list()\n",
    "X=[]\n",
    "\n",
    "def main(population_size,prob_cross,prob_mutation):\n",
    "  \n",
    "  \n",
    "  for i in range(10): # iterations\n",
    "      print(\"%d ITERATION \"%(i+1))\n",
    "\n",
    "       # dimiourgia plithismou\n",
    "      pop = toolbox.population(n=population_size)\n",
    "\n",
    "    #Evaluation fitness function \n",
    "      fitnesses = list(map(toolbox.evaluate, pop))\n",
    "      for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "   \n",
    "  \n",
    "    # CXPB pithanotita zeugaromatos\n",
    "    # MUTPB pithanotita metalakseis\n",
    "      CXPB, MUTPB = prob_cross, prob_mutation\n",
    "\n",
    "      #Statistics\n",
    "      stats = tools.Statistics(key=lambda ind:ind.fitness.values)\n",
    "      record = stats.compile(pop)\n",
    "      log = tools.Logbook()\n",
    "      log.record(gen=0, **record)\n",
    "\n",
    "\t# Extracting all the fitnesses of (epistrefi to fitness)\n",
    "      fits = [ind.fitness.values[0] for ind in pop]\n",
    "\t\n",
    "    # Variable keeping track of the number of generations\n",
    "    #metritis gia generation\n",
    "      generation=0\n",
    "      #Save to fitness pou exoume\n",
    "      previous_fit=max(fits)\n",
    "      #best fitness apo oles tis genies\n",
    "      bestfitness=0\n",
    "  #creteria \n",
    "      g = 1\n",
    "      critiria = False \n",
    "      max_g=50\n",
    "      fitness_unchanged = 0\n",
    "\n",
    "  # Begin the evolution\n",
    "      while critiria==False :\n",
    "        generation +=1\n",
    "\n",
    "    # A new generation\n",
    "        print(\"-- Generation %i --\" % generation) \n",
    "\n",
    "    # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "    # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring)) \n",
    "\n",
    "    # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < CXPB:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            if random.random() < MUTPB:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "    # Replace the old population by the offspring\n",
    "        pop[:] = offspring\n",
    "\n",
    "    \n",
    "    # Gather all the fitnesses in one list and print the stats\n",
    "        fitness = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "        record = stats.compile(pop)\n",
    "        log.record(gen=g, **record)  \n",
    "        best = log.select(\"max\")\n",
    "\n",
    "    #CRETERIA FOR WHILE LOOP(OSO TO TERM=FALSE trexei h while)\n",
    "        # reached max number of generations\n",
    "        if g >= max_g:\n",
    "            critiria = True\n",
    "            print(\"<-------------------ITERATION %d COMPLETED ----------------->\" %(i+1))\n",
    "        # best individual of gen is <1% better than best individual of previous gen\n",
    "        elif (g > 25) and max(fits) < (1.001*previous_fit):\n",
    "            critiria = True\n",
    "            print(\"<-------------------ITERATION %d COMPLETED ----------------->\" %(i+1))\n",
    "        # best individual of gen is same as best individual of previous gen\n",
    "        elif (g > 25) and previous_fit == max(fits):\n",
    "            fitness_unchanged += 1\n",
    "            #ean g>25 kai exw 5 fores stasimo fitness tote critiria=True\n",
    "            if fitness_unchanged >= 5:\n",
    "                critiria = True\n",
    "                print(\"<-------------------ITERATION %d COMPLETED ----------------->\" %(i+1))\n",
    "            else:\n",
    "                g += 1\n",
    "        # else continue\n",
    "        else:\n",
    "            fitness_unchanged = 0\n",
    "            g += 1\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "#------------------------------------EKTELESI GIA KATHE GENIA-----------------------------------------------------------------------------\n",
    "        #GIA KATHE GENERATION:\n",
    "        maxFitness_per_gen=(max(fitness))\n",
    "        average_fitness_per_gen = ((sum(fitness)/len(pop)))\n",
    "        print(\" Max fitness:%s && Average fitness:%s\" %((maxFitness_per_gen),(average_fitness_per_gen)))\n",
    "    \n",
    "\n",
    "#--------------------------------------TO MEGALITERO KATHE GENIAS KAI TO ANTISTOIXO AVG ----------------------------------------------------------------------\n",
    "        maxFitness_all_gen.append(maxFitness_per_gen)\n",
    "        averege_Fitness_all_gen.append(average_fitness_per_gen)\n",
    "\n",
    "        #TWRA THELOUME APO OLES TIS GENIES NA KRATISOUME MONO ENA, TO MEGALITERO \n",
    "        final_Bestfitness.append(max(maxFitness_all_gen))\n",
    "\n",
    "#----------------------------------------------SUNOLIKES GENIES EKTELESIS---------------------------------------------------------------------------------\n",
    "        Howmanygenerations.append(g)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        #PREPEI NA VROUME TO BEST FITNESS \n",
    "        best_fitness =max(fitness)\n",
    "        if best_fitness > bestfitness:\n",
    "            bestfitness = best_fitness\n",
    "            Bestindex =pop[fitness.index(best_fitness)]\n",
    "        BestFitnessPerGens.append(best_fitness)\n",
    "\n",
    "         \n",
    "        #Vale stin lista mono to megalutero stoixio pou vrikes\n",
    "        BestFitnessForAllGens.append(max(BestFitnessPerGens))\n",
    "        X.append(mean(BestFitnessPerGens))\n",
    "#################################################################################\n",
    "      \n",
    "\n",
    "################################################################################ \n",
    "\n",
    "  best_ind =tools.selBest(pop,1)[0]\n",
    "  print(\"<------------------------- APOTELESMATA ------------------------>\")\n",
    "  print(\"Avg for generations:\",mean(Howmanygenerations))\n",
    "  print(\"Best fitness of best individual:\",np.amax(max(BestFitnessForAllGens)))\n",
    "  print(\"Avg fitness of best individual:\",np.mean(BestFitnessForAllGens))\n",
    "\n",
    "\n",
    "  plt.figure(0)\n",
    "  plt.subplot(2, 2, 1)\n",
    "  plt.plot(BestFitnessForAllGens,color='red')\n",
    "  plt.title(\"MAX\")\n",
    "  plt.ylabel(\"Max fitness\")\n",
    "  plt.xlabel(\"Generation\")\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(2, 2, 2)\n",
    "  plt.plot(X,color=\"green\")\n",
    "  plt.title(\"AVG\")\n",
    "  plt.ylabel(\"Avg fitness\")\n",
    "  plt.xlabel(\"Generation\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  print(\"Best solution of best individual:\",(best_ind))\n",
    "\n",
    "  return best_ind\n",
    " \n",
    "keep = main(population_size,prob_cross,prob_mutation)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "tf_idf_Xtrain=tf_idf_Xtrain[:8251]\n",
    "labels_fnames = [\n",
    "            '/home/spetz/Downloads/DeliciousMIL/Data/train-label.dat',\n",
    "            ]\n",
    "y = pd.read_csv(labels_fnames[0] , delimiter = ' ', header = None)\n",
    "\n",
    "\n",
    "clean_doc = []\n",
    "wordfreq = {}\n",
    "for doc in file:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "            \n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "\n",
    "sentence_vectors = []\n",
    "for doc in file:\n",
    "    doc_tokens = nltk.word_tokenize(doc)\n",
    "    vec = []\n",
    "    for token in wordfreq:\n",
    "        if token in doc_tokens:\n",
    "            count = 0\n",
    "            for tok in doc_tokens:\n",
    "                if tok == token:\n",
    "                    count += 1\n",
    "            vec.append(count)\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    sentence_vectors.append(vec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8251, 8522)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train =np.array(sentence_vectors)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5775, 8522) (2476, 8522) (5775, 20) (2476, 20)\n"
     ]
    }
   ],
   "source": [
    "X_train.shape\n",
    "for i in range(len(keep)):\n",
    "    if keep[i] == 0:\n",
    "        X_train[:,i] = 0\n",
    "        \n",
    "       \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, y, test_size=0.3, random_state=0)  \n",
    "\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5775, 7565) (2476, 7565) (5775, 20) (2476, 20)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_minmax=X_train[:,:-957]\n",
    "X_test_minmax=X_test[:,:-957]\n",
    "print(X_train_minmax.shape,X_test_minmax.shape,Y_train.shape,Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 22:15:20.125348: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-02 22:15:20.125370: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-06-02 22:15:22.247850: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-02 22:15:22.247875: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-02 22:15:22.247892: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (spetz-ZenBook-UX434IQ-Q407IQ): /proc/driver/nvidia/version does not exist\n",
      "2022-06-02 22:15:22.248511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "model = tf.keras.models.load_model(\"/home/spetz/Downloads/ce.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for  retrained model is 0.3659127652645111\n",
      "Lose for retrained model is  0.5216124653816223\n"
     ]
    }
   ],
   "source": [
    "loss2, accuracy2 = model.evaluate(X_test_minmax, Y_test, verbose=0)\n",
    "print(\"Accuracy for  retrained model is\", accuracy2)\n",
    "print(\"Lose for retrained model is \",loss2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.layers import Dense,Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.optimizer_v1 import SGD\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(Dense(2896, activation='relu', input_shape=(5775,)))\n",
    "    model.add(Dense(20, activation='sigmoid'))\n",
    "    # optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.05,momentum = 0.6)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "def evaluate_model(X_train_normalized,Y_train,X_test_normalized,Y_test):            \n",
    "    fold_number2=0\n",
    "    fold_number = 0\n",
    "    sum_of_acc2 =0\n",
    "    sum_of_loss2 = 0\n",
    "    sum_of_acc=0\n",
    "    sum_of_loss=0\n",
    "    losses,scores,histories = list(),list(),list()\n",
    "    losses2,scores2,histories2 = list(),list(),list()\n",
    "    kfold = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    epochs = 30\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X_train_normalized,Y_train):  \n",
    "        ce = create_model()\n",
    "        #es=EarlyStopping(monitor='val_loss' , mode='min' , verbose=1)\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_train_normalized[train_index,:], X_train_normalized[test_index,:]\n",
    "        y_train, y_test = Y_train.iloc[train_index],Y_train.iloc[test_index]\n",
    "    \n",
    "        #MODEL for cross-entropy\n",
    "\n",
    "        history = ce.fit(X_train_normalized[train_index,:],Y_train.iloc[train_index] , epochs=epochs , validation_data=(X_test, y_test) ,verbose=1)\n",
    "        loss, val_acc = ce.evaluate(X_test_normalized,Y_test,verbose=1)\n",
    "        #MODEL 2 for Mse\n",
    "        savemodel=ce.save(\"ce.h5\")\n",
    "\t     \n",
    "\n",
    "        print(\"-\"*80)\n",
    "        ###########################\n",
    "        fold_number +=1 \n",
    "        fold_number2 +=1\n",
    "        ##########################\n",
    "        print(\" for cross entropy fold\",(fold_number),\"\\n|  loss:\" , loss, \"Accuracy:\",val_acc)\n",
    "        ##########################\n",
    "        sum_of_acc += val_acc\n",
    "        sum_of_loss += loss\n",
    "        #########################\n",
    "    \n",
    "\n",
    "        scores.append(val_acc)\n",
    "        histories.append(history)\n",
    "  \n",
    "\n",
    "        print(\"-\"*80)\n",
    "        print(\"\\n Cross-Entropy:the average of the loss and acc is: \\n\",\"loss:\" , sum_of_loss/fold_number, \"\\n\" , \"Accuracy\" , sum_of_acc/fold_number,\"\\n\")\n",
    "  \n",
    "        \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 22:21:00.249862: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 279602400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/145 [=====================>........] - ETA: 2s - loss: 0.4469 - accuracy: 0.2451"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/spetz/Desktop/my_final_ga.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/spetz/Desktop/my_final_ga.ipynb#ch0000014?line=0'>1</a>\u001b[0m evaluate_model(X_train_minmax,Y_train,X_test_minmax,Y_test)\n",
      "\u001b[1;32m/home/spetz/Desktop/my_final_ga.ipynb Cell 14'\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(X_train_normalized, Y_train, X_test_normalized, Y_test)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/spetz/Desktop/my_final_ga.ipynb#ch0000013?line=58'>59</a>\u001b[0m y_train, y_test \u001b[39m=\u001b[39m Y_train\u001b[39m.\u001b[39miloc[train_index],Y_train\u001b[39m.\u001b[39miloc[test_index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/spetz/Desktop/my_final_ga.ipynb#ch0000013?line=60'>61</a>\u001b[0m \u001b[39m#MODEL for cross-entropy\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/spetz/Desktop/my_final_ga.ipynb#ch0000013?line=62'>63</a>\u001b[0m history \u001b[39m=\u001b[39m ce\u001b[39m.\u001b[39;49mfit(X_train_normalized[train_index,:],Y_train\u001b[39m.\u001b[39;49miloc[train_index] , epochs\u001b[39m=\u001b[39;49mepochs , validation_data\u001b[39m=\u001b[39;49m(X_test, y_test) ,verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/spetz/Desktop/my_final_ga.ipynb#ch0000013?line=63'>64</a>\u001b[0m loss, val_acc \u001b[39m=\u001b[39m ce\u001b[39m.\u001b[39mevaluate(X_test_normalized,Y_test,verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/spetz/Desktop/my_final_ga.ipynb#ch0000013?line=64'>65</a>\u001b[0m \u001b[39m#MODEL 2 for Mse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1401'>1402</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1402'>1403</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1403'>1404</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1404'>1405</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1405'>1406</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1406'>1407</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1407'>1408</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1408'>1409</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1409'>1410</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/keras/engine/training.py?line=1410'>1411</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2449'>2450</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2450'>2451</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2451'>2452</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2452'>2453</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=2453'>2454</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1859'>1860</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1860'>1861</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1861'>1862</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1862'>1863</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1863'>1864</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1864'>1865</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=1865'>1866</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=494'>495</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=495'>496</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=508'>509</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py?line=509'>510</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///home/spetz/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_model(X_train_minmax,Y_train,X_test_minmax,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
