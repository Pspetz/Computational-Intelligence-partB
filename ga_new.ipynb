{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/spetz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2022-05-28 20:45:06.957797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-28 20:45:06.957820: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np \n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "from deap import creator\n",
    "from deap import base\n",
    "from deap import tools\n",
    "import tensorflow as tf\n",
    "\n",
    "from deap import algorithms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################TRAIN DATA#############################\n",
    "path = '/home/spetz/Downloads/DeliciousMIL/Data/train-data.dat'\n",
    "\n",
    "\n",
    "clean_files = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "file = open(path).readlines()\n",
    "len(file)\n",
    "\n",
    "#clear data\n",
    "clear_file=[]\n",
    "for i in range(len(file)):\n",
    "    x=re.sub('<.*?>','',file[i])\n",
    "    clear_file.append(x)\n",
    "\n",
    "clear_file=clear_file[:1000]\n",
    "\n",
    "#######X-TRAIN#######\n",
    "#sequences word\n",
    "clean_doc = []\n",
    "wordfreq = {}\n",
    "for doc in clear_file:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "\n",
    "import heapq\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf_values = {}\n",
    "for token in most_freq:\n",
    "    doc_containing_word = 0\n",
    "    for document in clear_file:\n",
    "        if token in nltk.word_tokenize(document):\n",
    "            doc_containing_word += 1\n",
    "    word_idf_values[token] = np.log(len(clear_file)/(1 + doc_containing_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tf_values = {}\n",
    "for token in most_freq:\n",
    "    sent_tf_vector = []\n",
    "    for document in clear_file:\n",
    "        doc_freq = 0\n",
    "        for word in nltk.word_tokenize(document):\n",
    "            if token == word:\n",
    "                  doc_freq += 1\n",
    "        word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    word_tf_values[token] = sent_tf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_values = []\n",
    "for token in word_tf_values.keys():\n",
    "    tfidf_sentences = []\n",
    "    for tf_sentence in word_tf_values[token]:\n",
    "        tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "        tfidf_sentences.append(tf_idf_score)\n",
    "    tfidf_values.append(tfidf_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = np.asarray(tfidf_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf_idf = np.transpose(tf_idf_model)\n",
    "X_train_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################TRAIN DATA#############################\n",
    "path1 = '/home/spetz/Downloads/DeliciousMIL/Data/test-data.dat'\n",
    "\n",
    "\n",
    "clean_files = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "file1 = open(path1).readlines()\n",
    "len(file)\n",
    "\n",
    "#clear data\n",
    "clear_file1=[]\n",
    "for i in range(len(file1)):\n",
    "    x=re.sub('<.*?>','',file1[i])\n",
    "    clear_file1.append(x)\n",
    "\n",
    "clear_file1=clear_file1[:1000]\n",
    "\n",
    "#######X-TRAIN#######\n",
    "#sequences word\n",
    "clean_doc = []\n",
    "wordfreq = {}\n",
    "for doc in clear_file:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "\n",
    "import heapq\n",
    "most_freq1 = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "\n",
    "\n",
    "word_idf_values1 = {}\n",
    "for token in most_freq1:\n",
    "    doc_containing_word = 0\n",
    "    for document in clear_file1:\n",
    "        if token in nltk.word_tokenize(document):\n",
    "            doc_containing_word += 1\n",
    "    word_idf_values1[token] = np.log(len(clear_file1)/(1 + doc_containing_word))\n",
    "\n",
    "word_tf_values1 = {}\n",
    "for token in most_freq1:\n",
    "    sent_tf_vector = []\n",
    "    for document in clear_file1:\n",
    "        doc_freq = 0\n",
    "        for word in nltk.word_tokenize(document):\n",
    "            if token == word:\n",
    "                  doc_freq += 1\n",
    "        word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    word_tf_values1[token] = sent_tf_vector\n",
    "\n",
    "tfidf_values1 = []\n",
    "for token in word_tf_values1.keys():\n",
    "    tfidf_sentences = []\n",
    "    for tf_sentence in word_tf_values1[token]:\n",
    "        tf_idf_score = tf_sentence * word_idf_values1[token]\n",
    "        tfidf_sentences.append(tf_idf_score)\n",
    "    tfidf_values1.append(tfidf_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_values1=np.asarray(tfidf_values1)\n",
    "X_test_tf_idf=np.transpose(tfidf_values1)\n",
    "X_test_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 200) (1000, 200) (1000, 20) (1000, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#load labels\n",
    "labels_fnames = [\n",
    "            '/home/spetz/Downloads/DeliciousMIL/Data/train-label.dat',\n",
    "            '/home/spetz/Downloads/DeliciousMIL/Data/test-label.dat'\n",
    "            ]\n",
    "\n",
    "Y_train = pd.read_csv(labels_fnames[0] ,nrows=1000 ,delimiter = ' ', header = None)\n",
    "Y_test= pd.read_csv(labels_fnames[1], nrows=1000,delimiter = ' ', header = None)\n",
    "\n",
    "\n",
    "#len(test_labels) 3983\n",
    "#len(train_labels) 8251\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_tf_idf = scaler.fit_transform(X_test_tf_idf)\n",
    "X_test_tf_idf = scaler.fit_transform(X_test_tf_idf)\n",
    "\n",
    "print(X_train_tf_idf.shape,X_test_tf_idf.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 20:51:17.532003: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-28 20:51:17.532027: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-28 20:51:17.532043: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (spetz-ZenBook-UX434IQ-Q407IQ): /proc/driver/nvidia/version does not exist\n",
      "2022-05-28 20:51:17.532254: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model=load_model('/home/spetz/Downloads/CE_model.h5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitness function\n",
    "def fitness(ind):\n",
    "    loss, accuracy = model.evaluate(X_test_tf_idf, Y_test, verbose=0)\n",
    "    # We want to penalty the individual depending on how many inputs he has.\n",
    "    # Less inputs means more fit individual.\n",
    "    # We also want the higher accuracy value, so the fitness value is a combination of both.\n",
    "    # Higher fitness_ind will mean more fit individual, because we are based on accurcy.\n",
    "    fitness_ind = accuracy - (np.count_nonzero(ind) / 1000) * 0.6\n",
    "    return fitness_ind,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and register functions ( code taken from the DEAP documentation)\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "toolbox = base.Toolbox()\n",
    "# Attribute generator\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "# Structure initializers. 784 because every individual is a string of 784 bits.\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, 1000)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
